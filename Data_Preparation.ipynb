{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorenzrossi/MachineLearning-CatsDogs-NN/blob/master/Data_Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download the Cats and Dogs Dataset\n",
        "\n",
        "The dataset used is the famous **Dogs vs Cats** dataset from Kaggle:\n",
        "- **Kaggle Competition**: https://www.kaggle.com/c/dogs-vs-cats\n",
        "- **Direct Dataset Link**: https://www.kaggle.com/c/dogs-vs-cats/data\n",
        "\n",
        "The dataset contains 25,000 images of cats and dogs (12,500 each).\n",
        "\n",
        "### Option 1: Download using Kaggle API (Recommended)\n",
        "\n",
        "To use this method, you need to:\n",
        "1. Go to https://www.kaggle.com/account and create an API token\n",
        "2. Upload the `kaggle.json` file to your Colab session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect if running in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Setup Kaggle API credentials check FIRST (before installing/importing kaggle)\n",
        "import os\n",
        "DOWNLOAD_DATASET = False\n",
        "kaggle_json_path = None\n",
        "\n",
        "if IN_COLAB:\n",
        "    # In Colab, we'll check after upload\n",
        "    kaggle_json_path = os.path.join(os.path.expanduser('~'), '.kaggle', 'kaggle.json')\n",
        "    # Assume we'll upload it\n",
        "    DOWNLOAD_DATASET = True\n",
        "else:\n",
        "    # For local systems, check if kaggle.json exists BEFORE trying to use kaggle\n",
        "    kaggle_dir = os.path.join(os.path.expanduser('~'), '.kaggle')\n",
        "    kaggle_json_path = os.path.join(kaggle_dir, 'kaggle.json')\n",
        "    os.makedirs(kaggle_dir, exist_ok=True)\n",
        "    \n",
        "    if os.path.exists(kaggle_json_path):\n",
        "        print(f\"✓ Found Kaggle credentials at: {kaggle_json_path}\")\n",
        "        DOWNLOAD_DATASET = True\n",
        "    else:\n",
        "        print(\"=\" * 60)\n",
        "        print(\"⚠️  Kaggle API credentials not found!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"\\nTo download the dataset, you need to:\")\n",
        "        print(f\"1. Go to https://www.kaggle.com/account\")\n",
        "        print(f\"2. Scroll down to 'API' section\")\n",
        "        print(f\"3. Click 'Create New API Token' to download kaggle.json\")\n",
        "        print(f\"4. Place it at: {kaggle_json_path}\")\n",
        "        print(f\"\\nAlternatively, you can:\")\n",
        "        print(f\"- Skip this cell and manually download the dataset\")\n",
        "        print(f\"- Use the pre-prepared pickle files from Google Drive (see README.md)\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"\\n⚠️  Skipping dataset download. You can process existing data in the next cell.\")\n",
        "        DOWNLOAD_DATASET = False\n",
        "\n",
        "# Only install/use Kaggle if we have credentials or are in Colab\n",
        "if DOWNLOAD_DATASET:\n",
        "    # Install Kaggle API\n",
        "    if IN_COLAB:\n",
        "        !pip install -q kaggle\n",
        "        # Upload your kaggle.json file (run this cell and upload the file when prompted)\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload()\n",
        "        \n",
        "        # Move kaggle.json to the correct location\n",
        "        !mkdir -p ~/.kaggle\n",
        "        !mv kaggle.json ~/.kaggle/\n",
        "        !chmod 600 ~/.kaggle/kaggle.json\n",
        "    else:\n",
        "        import subprocess\n",
        "        import sys\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"kaggle\"], \n",
        "                                stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        except:\n",
        "            pass  # Kaggle might already be installed\n",
        "\n",
        "# Download the dataset (only if credentials are available)\n",
        "if DOWNLOAD_DATASET:\n",
        "    try:\n",
        "        if IN_COLAB:\n",
        "            !kaggle competitions download -c dogs-vs-cats\n",
        "            download_dir = '/content'\n",
        "            base_dir = '/content/drive/MyDrive/CatsDogs'\n",
        "        else:\n",
        "            import subprocess\n",
        "            result = subprocess.run(['kaggle', 'competitions', 'download', '-c', 'dogs-vs-cats'], \n",
        "                                  capture_output=True, text=True)\n",
        "            if result.returncode != 0:\n",
        "                print(f\"Error downloading dataset: {result.stderr}\")\n",
        "                raise Exception(\"Kaggle download failed\")\n",
        "            # Use current directory for local systems\n",
        "            download_dir = os.getcwd()\n",
        "            base_dir = os.path.join(os.getcwd(), 'CatsDogs')\n",
        "        \n",
        "        # Unzip the training data\n",
        "        import zipfile\n",
        "        import shutil\n",
        "        \n",
        "        # Unzip dogs-vs-cats.zip\n",
        "        zip_path = os.path.join(download_dir, 'dogs-vs-cats.zip')\n",
        "        if os.path.exists(zip_path):\n",
        "            print(\"Extracting dogs-vs-cats.zip...\")\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(download_dir)\n",
        "        \n",
        "        # Unzip train.zip\n",
        "        train_zip_path = os.path.join(download_dir, 'train.zip')\n",
        "        train_dir = os.path.join(download_dir, 'train')\n",
        "        if os.path.exists(train_zip_path):\n",
        "            print(\"Extracting train.zip...\")\n",
        "            with zipfile.ZipFile(train_zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(download_dir)\n",
        "        \n",
        "        # Create the directory structure expected by the code\n",
        "        os.makedirs(os.path.join(base_dir, 'Cats'), exist_ok=True)\n",
        "        os.makedirs(os.path.join(base_dir, 'Dogs'), exist_ok=True)\n",
        "        os.makedirs(os.path.join(base_dir, 'Pickles'), exist_ok=True)\n",
        "        \n",
        "        # Organize images into Cats and Dogs folders\n",
        "        import glob\n",
        "        \n",
        "        if os.path.exists(train_dir):\n",
        "            print(\"Organizing images into Cats and Dogs folders...\")\n",
        "            # Move cat images (files starting with 'cat.')\n",
        "            cat_files = glob.glob(os.path.join(train_dir, 'cat.*'))\n",
        "            for file in cat_files:\n",
        "                shutil.move(file, os.path.join(base_dir, 'Cats'))\n",
        "            \n",
        "            # Move dog images (files starting with 'dog.')\n",
        "            dog_files = glob.glob(os.path.join(train_dir, 'dog.*'))\n",
        "            for file in dog_files:\n",
        "                shutil.move(file, os.path.join(base_dir, 'Dogs'))\n",
        "            \n",
        "            print(\"Dataset downloaded and organized successfully!\")\n",
        "            print(f\"Base directory: {base_dir}\")\n",
        "            print(f\"Number of cat images: {len(os.listdir(os.path.join(base_dir, 'Cats')))}\")\n",
        "            print(f\"Number of dog images: {len(os.listdir(os.path.join(base_dir, 'Dogs')))}\")\n",
        "        else:\n",
        "            print(f\"⚠️  Training directory not found at {train_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  Error during download: {e}\")\n",
        "        print(\"You can skip this step and process existing data in the next cell.\")\n",
        "else:\n",
        "    # Set base directory even if not downloading\n",
        "    if IN_COLAB:\n",
        "        base_dir = '/content/drive/MyDrive/CatsDogs'\n",
        "    else:\n",
        "        base_dir = os.path.join(os.getcwd(), 'CatsDogs')\n",
        "    \n",
        "    # Create directory structure\n",
        "    os.makedirs(os.path.join(base_dir, 'Cats'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(base_dir, 'Dogs'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(base_dir, 'Pickles'), exist_ok=True)\n",
        "    print(f\"\\nBase directory set to: {base_dir}\")\n",
        "    print(\"You can process existing data in the next cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 2: Direct Download (Alternative method)\n",
        "\n",
        "If you prefer not to use Kaggle API, you can download the dataset manually:\n",
        "1. Visit https://www.kaggle.com/c/dogs-vs-cats/data\n",
        "2. Download `train.zip` (requires Kaggle account)\n",
        "3. Extract it and organize images:\n",
        "   - **For Google Colab**: Extract to `/content/drive/MyDrive/CatsDogs/` and organize into `Cats/` and `Dogs/` subdirectories\n",
        "   - **For Local Systems**: Extract to a `CatsDogs/` folder in your current directory and organize into `Cats/` and `Dogs/` subdirectories\n",
        "\n",
        "**Note**: The dataset requires a Kaggle account to download. If you already have the dataset organized, you can skip the download cells above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8cP3Kq5ohHr",
        "outputId": "a918d268-f936-454a-d46f-f78a20d3243a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "24978\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from os import listdir\n",
        "import pandas as pd\n",
        "# Basic packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "# Image preprocessing\n",
        "import pathlib\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import cv2\n",
        "\n",
        "# Detect if running in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Use Google Drive path for Colab\n",
        "    base_dir = '/content/drive/MyDrive/CatsDogs'\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    # Use current directory for local systems (Windows, Mac, Linux)\n",
        "    base_dir = os.path.join(os.getcwd(), 'CatsDogs')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "os.makedirs(os.path.join(base_dir, 'Cats'), exist_ok=True)\n",
        "os.makedirs(os.path.join(base_dir, 'Dogs'), exist_ok=True)\n",
        "os.makedirs(os.path.join(base_dir, 'Pickles'), exist_ok=True)\n",
        "\n",
        "print(f\"Working directory: {base_dir}\")\n",
        "print(f\"Running in Colab: {IN_COLAB}\")\n",
        "\n",
        "# Import of Data\n",
        "\n",
        "# I define some parameters in order to resize images in a uniform way\n",
        "# Note: I set channels = 1 for grayscale images (using IMREAD_GRAYSCALE)\n",
        "# If you want RGB images, change channels to 3 and use cv2.IMREAD_COLOR instead\n",
        "# The images will be normalized to [0, 1] range by dividing by 255.0 (important for neural networks)\n",
        "img_width = 100\n",
        "img_height = 100\n",
        "img_size = (img_width, img_height)\n",
        "channels = 1  # 1 for grayscale, 3 for RGB\n",
        "\n",
        "# Pets is the list in which all the images will be stored\n",
        "pets = []\n",
        "\n",
        "# I define the two categories of pets (related to the sub-directories)\n",
        "categories = ['Cats','Dogs']\n",
        "\n",
        "# I create a function in order to load the data, transform them into arrays, assign them to a class and store them in the pets list\n",
        "def create_data():\n",
        "    # Determine OpenCV read mode based on channels\n",
        "    read_mode = cv2.IMREAD_GRAYSCALE if channels == 1 else cv2.IMREAD_COLOR\n",
        "    \n",
        "    print(f\"Loading images from {base_dir}...\")\n",
        "    print(f\"Image size: {img_size}, Channels: {channels}\")\n",
        "    \n",
        "    # the function iterates through the two sub-directories\n",
        "    for category in categories:\n",
        "        path = os.path.join(base_dir, category)\n",
        "        \n",
        "        # Check if directory exists\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"Warning: Directory {path} does not exist. Please download the dataset first.\")\n",
        "            continue\n",
        "\n",
        "        # assign 0 to cat and 1 to dog, according to the index of the categories\n",
        "        pet_class = categories.index(category)\n",
        "\n",
        "        # Filter image files by extension (only process valid image files)\n",
        "        image_files = [f for f in os.listdir(path) \n",
        "                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        \n",
        "        print(f\"Processing {len(image_files)} {category.lower()} images...\")\n",
        "\n",
        "        # the function then iterates through each image in both the folders. I used the opencv (cv2 when imported) package to read and load them. \n",
        "        # IMREAD_COLOR keeps the color in the image, whereas IMREAD_GRAYSCALE would turn them into grayscale images.\n",
        "        # The function resizes the images as defined by the previous parameters and finally appends the array of the image, \n",
        "        # together with the class value (creating a sub-list of two values for each image), to the pets list.\n",
        "        # The function skips those images which seem broken.\n",
        "        for img_file in image_files:\n",
        "            img_path = os.path.join(path, img_file)\n",
        "            try:\n",
        "                img_array = cv2.imread(img_path, read_mode)\n",
        "                # Check if image was loaded successfully\n",
        "                if img_array is None:\n",
        "                    continue\n",
        "                # Resize image\n",
        "                resized = cv2.resize(img_array, img_size)\n",
        "                pets.append([resized, pet_class])\n",
        "            except Exception as e:\n",
        "                # Skip broken images silently\n",
        "                pass\n",
        "\n",
        "create_data()\n",
        "\n",
        "print(f\"Total number of images loaded: {len(pets)}\")\n",
        "\n",
        "# since the previous function iterates linearly through the two sub-directories, I shuffle the data in order to mix the order of the images\n",
        "print(\"Shuffling data...\")\n",
        "random.shuffle(pets)\n",
        "\n",
        "# split the data arrays and the labels for the classification task\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for image, label in pets:\n",
        "    X.append(image)\n",
        "    y.append(label)\n",
        "\n",
        "# Convert to numpy arrays with proper dtypes\n",
        "X = np.array(X, dtype=np.float64)\n",
        "X = X.reshape(-1, img_height, img_width, channels)\n",
        "y = np.array(y, dtype=np.int32)\n",
        "\n",
        "# Normalize pixel values to [0, 1] range (important for neural networks)\n",
        "print(\"Normalizing pixel values to [0, 1] range...\")\n",
        "X = X / 255.0\n",
        "\n",
        "print(f\"Data shape: X = {X.shape}, y = {y.shape}\")\n",
        "print(f\"X dtype: {X.dtype}, y dtype: {y.dtype}\")\n",
        "print(f\"X range: [{X.min():.3f}, {X.max():.3f}]\")\n",
        "\n",
        "# Save the processed data as pickle files\n",
        "pickles_dir = os.path.join(base_dir, 'Pickles')\n",
        "x_pickle_path = os.path.join(pickles_dir, 'X.pickle')\n",
        "y_pickle_path = os.path.join(pickles_dir, 'y.pickle')\n",
        "\n",
        "print(f\"\\nSaving X to {x_pickle_path}...\")\n",
        "with open(x_pickle_path, 'wb') as f:\n",
        "    pickle.dump(X, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(f\"Saving y to {y_pickle_path}...\")\n",
        "with open(y_pickle_path, 'wb') as f:\n",
        "    pickle.dump(y, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Verify saved files\n",
        "file_size_x = os.path.getsize(x_pickle_path) / (1024 * 1024)  # MB\n",
        "file_size_y = os.path.getsize(y_pickle_path) / (1024 * 1024)  # MB\n",
        "\n",
        "print(f\"\\nPickle files saved successfully!\")\n",
        "print(f\"X.pickle size: {file_size_x:.2f} MB\")\n",
        "print(f\"y.pickle size: {file_size_y:.2f} MB\")\n",
        "\n",
        "# Load the pickled data to verify\n",
        "print(\"\\nVerifying saved files...\")\n",
        "with open(x_pickle_path, 'rb') as f:\n",
        "    X_loaded = pickle.load(f)\n",
        "\n",
        "with open(y_pickle_path, 'rb') as f:\n",
        "    y_loaded = pickle.load(f)\n",
        "\n",
        "print(f\"Verification successful!\")\n",
        "print(f\"Loaded data: X = {X_loaded.shape}, y = {y_loaded.shape}\")\n",
        "print(f\"X dtype: {X_loaded.dtype}, y dtype: {y_loaded.dtype}\")\n",
        "print(f\"X range: [{X_loaded.min():.3f}, {X_loaded.max():.3f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36XOqjzxHNgB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPzMc0GrBGagUgDwi4QR6dP",
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
